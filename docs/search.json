[
  {
    "objectID": "index.html#session-overview",
    "href": "index.html#session-overview",
    "title": "Applied Mathematics in the Age of AI",
    "section": "Session Overview",
    "text": "Session Overview\nThis intensive 4-hour session is designed for mathematics PG students and faculty to rigorously explore the discipline of Data Science through the lens of Applied Mathematics and modern computation.\nWe will move beyond a high-level overview to establish a strong theoretical and practical foundation, demonstrating how core mathematical principles—from Linear Algebra and Optimization to Probability and Statistics—form the engine of today’s most powerful AI systems."
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Applied Mathematics in the Age of AI",
    "section": "Target Audience",
    "text": "Target Audience\nPostgraduate Students in Mathematics, Statistics, and related fields.\nFaculty members seeking to integrate modern Data Science curriculum into their teaching and research."
  },
  {
    "objectID": "index.html#session-mode",
    "href": "index.html#session-mode",
    "title": "Applied Mathematics in the Age of AI",
    "section": "Session Mode",
    "text": "Session Mode\nThe session is structured in an interactive mode, blending in-depth mathematical derivations with live computational demonstrations using industry-standard tools (e.g., Python/R environments).\nWe will emphasize:\n\nMathematical Derivations: Understanding the why behind algorithms.\nComputational Execution: Translating theory into working code.\nCritical Analysis: Evaluating model performance based on mathematical metrics."
  },
  {
    "objectID": "index.html#key-learning-outcome",
    "href": "index.html#key-learning-outcome",
    "title": "Applied Mathematics in the Age of AI",
    "section": "Key Learning Outcome",
    "text": "Key Learning Outcome\nParticipants will gain a clear understanding of how their mathematical expertise directly translates into practical, state-of-the-art computational modeling, identifying immediate pathways to apply these tools in academic research and industry.\nNavigate to the Key Topics & Schedule tab to see the detailed breakdown of the 4-hour session."
  },
  {
    "objectID": "03-Basic-python.html",
    "href": "03-Basic-python.html",
    "title": "Module 0 — Introduction to Visualization",
    "section": "",
    "text": "To study and implement basic numerical computation, symbolic mathematics and graphical visualization using Python libraries such as NumPy, SymPy and Matplotlib.\n\n\nNumPy (Numerical Python) is the foundational library for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. In this section, basic mathematical operations are transleted to python. For this simulation examples Numpy library will be used.\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\n\n\nFormatting output\n\n\n\nIn-order to handle exceptions on division operation, usually python make even zero output to a very small positive number.\n\n\n\n\n\nNumPy allows us to perform element-wise mathematical operations on arrays without using loops. &gt;Simulating a basic mathematics functions, \\(y=f(x)\\) in data driven approach\n\n\nCode\n# Sine function\nangles = np.array([0, np.pi/2, np.pi])\nsin_values = np.sin(angles)\nprint(\"Sine Values:\", sin_values)\n\n# Exponential function\na=np.array([1,2,3])\nexp_values = np.exp(a)\nprint(\"Exponential Values:\", exp_values)\n\n# Square root function\nsqrt_values = np.sqrt(a)\nprint(\"Square Root Values:\", sqrt_values)\n\n\nSine Values: [0.0000000e+00 1.0000000e+00 1.2246468e-16]\nExponential Values: [ 2.71828183  7.3890561  20.08553692]\nSquare Root Values: [1.         1.41421356 1.73205081]\n\n\n\n\n\nUnderstanding how to structure data is crucial. We can reshape arrays to change their dimensions or access specific elements using indexing and slicing.\n\n\nCode\n# Reshaping a 1D array to 2D\nreshaped_array = np.arange(6).reshape((2, 3))\nprint(\"Reshaped Array:\\n\", reshaped_array)\n\n# Indexing and Slicing\narray = np.array([1, 2, 3, 4, 5])\n\n# Accessing an element\nprint(\"Element at index 2:\", array[2])\n\n# Slicing (elements from index 1 up to, but not including, 4)\nprint(\"Elements from index 1 to 3:\", array[1:4])\n\n# Fancy indexing (accessing specific non-consecutive indices)\nindices = [0, 2, 4]\nprint(\"Elements at indices 0, 2, 4:\", array[indices])\n\n\nReshaped Array:\n [[0 1 2]\n [3 4 5]]\nElement at index 2: 3\nElements from index 1 to 3: [2 3 4]\nElements at indices 0, 2, 4: [1 3 5]\n\n\n\n\n\nThese functions allow us to summarize data by computing statistics like sums, means, or finding extreme values.\n\n\nCode\n# Sum of elements\nsum_value = np.sum(array)\nprint(\"Sum:\", sum_value)\n\n# Mean of elements\nmean_value = np.mean(array)\nprint(\"Mean:\", mean_value)\n\n# Maximum and Minimum values\nprint(\"Max:\", np.max(array))\nprint(\"Min:\", np.min(array))\n\n\nSum: 15\nMean: 3.0\nMax: 5\nMin: 1\n\n\n\n\n\nBroadcasting is a powerful mechanism that allows NumPy to work with arrays of different shapes during arithmetic operations. It automatically “stretches” the smaller array to match the larger one.\n\n\nCode\n# Broadcasting a scalar value\nscalar = 5\nbroadcasted_sum = array + scalar\nprint(\"Array + Scalar:\\n\", broadcasted_sum)\n\n# Broadcasting with different shapes\narray_2d = np.array([[1, 2, 3], [4, 5, 6]])\narray_1d = np.array([1, 2, 3])\nbroadcasted_sum_2d = array_2d + array_1d\nprint(\"2D Array + 1D Array:\\n\", broadcasted_sum_2d)\n\n\nArray + Scalar:\n [ 6  7  8  9 10]\n2D Array + 1D Array:\n [[2 4 6]\n [5 7 9]]\n\n\n\n\n\nUnlike NumPy, which computes numerical approximations, SymPy is used for symbolic mathematics. It handles algebra, calculus, and equation solving exactly, much like we would on paper.\n\n\nWe start by defining symbolic variables and manipulating algebraic expressions.\n\n\nCode\nimport sympy as sp\n\n# Define symbols\nx, y = sp.symbols('x y')\n\n# Define an expression\nexpr = x**2 + 2*x + 1\n\n# Simplify the expression\nsimplified_expr = sp.simplify(expr)\nprint(f\"Simplified expression: {simplified_expr}\")\n\n# Expand the expression\nexpanded_expr = sp.expand((x + 1)**2)\nprint(f\"Expanded expression: {expanded_expr}\")\n\n\nSimplified expression: x**2 + 2*x + 1\nExpanded expression: x**2 + 2*x + 1\n\n\n\n\n\n\nSymPy can find exact roots of algebraic equations.\n\n\nCode\n# Define equation: x^2 - 4 = 0\neq = sp.Eq(x**2 - 4, 0)\n\n# Solve for x\nsolutions = sp.solve(eq, x)\nprint(f\"Solutions: {solutions}\")\n\n\nSolutions: [-2, 2]\n\n\n\n\n\nSymPy excels at performing derivatives and integrals symbolically.\n\n\nCode\n# Define a function\nf = sp.sin(x) * sp.exp(x)\n\n# Differentiate the function\nf_prime = sp.diff(f, x)\nprint(f\"Derivative: {f_prime}\")\n\n# Integrate the function\nintegral = sp.integrate(f, x)\nprint(f\"Integral: {integral}\")\n\n\nDerivative: exp(x)*sin(x) + exp(x)*cos(x)\nIntegral: exp(x)*sin(x)/2 - exp(x)*cos(x)/2\n\n\n\n\n\nWe can also compute limits and Taylor series expansions, which are essential for approximation theory.\n\n\nCode\n# Limit of sin(x)/x as x approaches 0\nf_limit = sp.sin(x) / x\nlimit_val = sp.limit(f_limit, x, 0)\nprint(f\"Limit: {limit_val}\")\n\n# Series expansion of e^x around x=0\nf_exp = sp.exp(x)\nseries_f = sp.series(f_exp, x, 0, 6)\nprint(f\"Series expansion: {series_f}\")\n\n\nLimit: 1\nSeries expansion: 1 + x + x**2/2 + x**3/6 + x**4/24 + x**5/120 + O(x**6)"
  },
  {
    "objectID": "03-Basic-python.html#aim",
    "href": "03-Basic-python.html#aim",
    "title": "Module 0 — Introduction to Visualization",
    "section": "",
    "text": "To study and implement basic numerical computation, symbolic mathematics and graphical visualization using Python libraries such as NumPy, SymPy and Matplotlib.\n\n\nNumPy (Numerical Python) is the foundational library for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. In this section, basic mathematical operations are transleted to python. For this simulation examples Numpy library will be used.\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\n\n\nFormatting output\n\n\n\nIn-order to handle exceptions on division operation, usually python make even zero output to a very small positive number.\n\n\n\n\n\nNumPy allows us to perform element-wise mathematical operations on arrays without using loops. &gt;Simulating a basic mathematics functions, \\(y=f(x)\\) in data driven approach\n\n\nCode\n# Sine function\nangles = np.array([0, np.pi/2, np.pi])\nsin_values = np.sin(angles)\nprint(\"Sine Values:\", sin_values)\n\n# Exponential function\na=np.array([1,2,3])\nexp_values = np.exp(a)\nprint(\"Exponential Values:\", exp_values)\n\n# Square root function\nsqrt_values = np.sqrt(a)\nprint(\"Square Root Values:\", sqrt_values)\n\n\nSine Values: [0.0000000e+00 1.0000000e+00 1.2246468e-16]\nExponential Values: [ 2.71828183  7.3890561  20.08553692]\nSquare Root Values: [1.         1.41421356 1.73205081]\n\n\n\n\n\nUnderstanding how to structure data is crucial. We can reshape arrays to change their dimensions or access specific elements using indexing and slicing.\n\n\nCode\n# Reshaping a 1D array to 2D\nreshaped_array = np.arange(6).reshape((2, 3))\nprint(\"Reshaped Array:\\n\", reshaped_array)\n\n# Indexing and Slicing\narray = np.array([1, 2, 3, 4, 5])\n\n# Accessing an element\nprint(\"Element at index 2:\", array[2])\n\n# Slicing (elements from index 1 up to, but not including, 4)\nprint(\"Elements from index 1 to 3:\", array[1:4])\n\n# Fancy indexing (accessing specific non-consecutive indices)\nindices = [0, 2, 4]\nprint(\"Elements at indices 0, 2, 4:\", array[indices])\n\n\nReshaped Array:\n [[0 1 2]\n [3 4 5]]\nElement at index 2: 3\nElements from index 1 to 3: [2 3 4]\nElements at indices 0, 2, 4: [1 3 5]\n\n\n\n\n\nThese functions allow us to summarize data by computing statistics like sums, means, or finding extreme values.\n\n\nCode\n# Sum of elements\nsum_value = np.sum(array)\nprint(\"Sum:\", sum_value)\n\n# Mean of elements\nmean_value = np.mean(array)\nprint(\"Mean:\", mean_value)\n\n# Maximum and Minimum values\nprint(\"Max:\", np.max(array))\nprint(\"Min:\", np.min(array))\n\n\nSum: 15\nMean: 3.0\nMax: 5\nMin: 1\n\n\n\n\n\nBroadcasting is a powerful mechanism that allows NumPy to work with arrays of different shapes during arithmetic operations. It automatically “stretches” the smaller array to match the larger one.\n\n\nCode\n# Broadcasting a scalar value\nscalar = 5\nbroadcasted_sum = array + scalar\nprint(\"Array + Scalar:\\n\", broadcasted_sum)\n\n# Broadcasting with different shapes\narray_2d = np.array([[1, 2, 3], [4, 5, 6]])\narray_1d = np.array([1, 2, 3])\nbroadcasted_sum_2d = array_2d + array_1d\nprint(\"2D Array + 1D Array:\\n\", broadcasted_sum_2d)\n\n\nArray + Scalar:\n [ 6  7  8  9 10]\n2D Array + 1D Array:\n [[2 4 6]\n [5 7 9]]\n\n\n\n\n\nUnlike NumPy, which computes numerical approximations, SymPy is used for symbolic mathematics. It handles algebra, calculus, and equation solving exactly, much like we would on paper.\n\n\nWe start by defining symbolic variables and manipulating algebraic expressions.\n\n\nCode\nimport sympy as sp\n\n# Define symbols\nx, y = sp.symbols('x y')\n\n# Define an expression\nexpr = x**2 + 2*x + 1\n\n# Simplify the expression\nsimplified_expr = sp.simplify(expr)\nprint(f\"Simplified expression: {simplified_expr}\")\n\n# Expand the expression\nexpanded_expr = sp.expand((x + 1)**2)\nprint(f\"Expanded expression: {expanded_expr}\")\n\n\nSimplified expression: x**2 + 2*x + 1\nExpanded expression: x**2 + 2*x + 1\n\n\n\n\n\n\nSymPy can find exact roots of algebraic equations.\n\n\nCode\n# Define equation: x^2 - 4 = 0\neq = sp.Eq(x**2 - 4, 0)\n\n# Solve for x\nsolutions = sp.solve(eq, x)\nprint(f\"Solutions: {solutions}\")\n\n\nSolutions: [-2, 2]\n\n\n\n\n\nSymPy excels at performing derivatives and integrals symbolically.\n\n\nCode\n# Define a function\nf = sp.sin(x) * sp.exp(x)\n\n# Differentiate the function\nf_prime = sp.diff(f, x)\nprint(f\"Derivative: {f_prime}\")\n\n# Integrate the function\nintegral = sp.integrate(f, x)\nprint(f\"Integral: {integral}\")\n\n\nDerivative: exp(x)*sin(x) + exp(x)*cos(x)\nIntegral: exp(x)*sin(x)/2 - exp(x)*cos(x)/2\n\n\n\n\n\nWe can also compute limits and Taylor series expansions, which are essential for approximation theory.\n\n\nCode\n# Limit of sin(x)/x as x approaches 0\nf_limit = sp.sin(x) / x\nlimit_val = sp.limit(f_limit, x, 0)\nprint(f\"Limit: {limit_val}\")\n\n# Series expansion of e^x around x=0\nf_exp = sp.exp(x)\nseries_f = sp.series(f_exp, x, 0, 6)\nprint(f\"Series expansion: {series_f}\")\n\n\nLimit: 1\nSeries expansion: 1 + x + x**2/2 + x**3/6 + x**4/24 + x**5/120 + O(x**6)"
  },
  {
    "objectID": "03-Basic-python.html#graphical-visualization-with-matplotlib",
    "href": "03-Basic-python.html#graphical-visualization-with-matplotlib",
    "title": "Module 0 — Introduction to Visualization",
    "section": "Graphical Visualization with Matplotlib",
    "text": "Graphical Visualization with Matplotlib\nMatplotlib is the standard library for creating static, animated, and interactive visualizations in Python.\n\nBasic Plots (Line and Scatter)\nLine plots are used for continuous data, while scatter plots are best for discrete data points.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Data\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# Basic Line Plot\nplt.plot(x, y)\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\nplt.title('Basic Line Plot')\nplt.show()\n\n# Basic Scatter Plot\nplt.scatter(x, y)\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\nplt.title('Basic Scatter Plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical and Distribution Plots\nBar charts compare categories, while histograms show the frequency distribution of continuous data.\n\n\nCode\n# Bar Plot\ncategories = ['A', 'B', 'C', 'D']\nvalues = [4, 7, 1, 8]\nplt.bar(categories, values)\nplt.title('Basic Bar Plot')\nplt.show()\n\n# Histogram\ndata = np.random.randn(1000)\nplt.hist(data, bins=30, edgecolor=\"black\")\nplt.title('Basic Histogram')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPie Chart and Customization\nPie charts visualize proportions. You can also heavily customize plots (colors, line styles, markers) to improve readability.\n\n\nCode\n# Pie Chart\nlabels = ['A', 'B', 'C', 'D']\nsizes = [15, 30, 45, 10]\nexplode = (0.1, 0, 0, 0) # explode 1st slice\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True)\nplt.title('Basic Pie Chart')\nplt.show()\n\n# Customized Line Plot\nplt.plot(x, y, color='green', marker='o', linestyle='dashed', linewidth=2, markersize=12)\nplt.title('Customized Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving linear system graphically\n\n\nCode\n## visualization as intersection of line joining\nimport matplotlib.pyplot as plt\nx = np.arange(-10, 10)\ny = 2*x\ny1 = -x + 3\nplt.figure()\nplt.plot(x, y)\nplt.plot(x, y1)\nplt.xlim(0, 3)\nplt.ylim(0, 3)\n# draw axes\nplt.axvline(x=0, color='grey')\nplt.axhline(y=0, color='grey')\nplt.plot([1,1],[0,2], 'g--')\nplt.plot([1,0],[2,2], 'g--')\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\n\n\n\nFundementals of Digital Images\n\nPlotting a matrix as an image This is a matrix which is an image. Here each square corresponds to one item in the matrix, and color of each square corresponds to the value of element in the matrix.\n\n\n\nCode\nI=np.matrix([[1,1,1,1],[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]])\nprint(I)\nplt.imshow(I,cmap='gray', vmin = 0, vmax = 1,interpolation='none')\nplt.show()\n\n\n[[1 1 1 1]\n [0 0 0 1]\n [0 0 1 0]\n [0 1 0 0]\n [1 0 0 0]]\n\n\n\n\n\n\n\n\n\n\n\nCreating a random grayscale image\n\n\nCode\nV=np.random.randint(1,255,size=(5,4))\nprint(I+V)\nplt.imshow(I+V,cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n[[ 42  97 121 176]\n [248   5 160 158]\n [ 41  70  23 247]\n [200  41 214  29]\n [ 21  85  59 183]]\n\n\n\n\n\n\n\n\n\n\n\nBasic image Processing and Matrix operations\n\n\nCode\n##loading an image and show it using matrices of pixel values\nfrom skimage import io\nf = \"http://lenna.org/len_top.jpg\" #url of the image\na = io.imread(f) # read the image to a tensor\nc1=a[:,:,0] # channel 1\nc2=a[:,:,1] # channel 2\nc3=a[:,:,2] # channel 3\nprint(c1)\n\n\n[[109 109 108 ...  54  60  67]\n [112 111 107 ...  52  55  61]\n [111 110 107 ...  51  54  60]\n ...\n [130 129 133 ... 122 119 125]\n [128 127 132 ... 125 119 123]\n [133 130 127 ... 139 133 140]]\n\n\n\n\nFrequency distribution of channel 1 pixel values\n\n\nCode\nimport pandas as pd\nc1_array=np.array(list(c1)).reshape(-1)\npd.Series(c1_array).hist()\n\n\n\n\n\n\n\n\n\n\n\nAnother method to load the colour image and convert to grayscale\n\n\nCode\nfrom PIL import Image\nimport urllib.request\nurllib.request.urlretrieve(\n  'http://lenna.org/len_top.jpg',\n   \"input.jpg\")\n  \nimg = Image.open(\"input.jpg\")\n\n\n\n\nCode\nimggray = img.convert('LA')\nplt.figure(figsize=(8,6))\nplt.imshow(imggray);\n\n\n\n\n\n\n\n\n\n\n\nCode\nimgmat = np.array(list(imggray.getdata(band=0)), float)\nA=pd.Series(imgmat)\nA.hist(bins=20)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(12, 3))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nax1.imshow(c1, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nax2.imshow(c2, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nax3.imshow(c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## an application of matrix addition\nplt.imshow(0.34*c1+0.2*c2-0.0001*c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## an application of matrix multiplication\nplt.imshow(np.dot(c1,c2.T), cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## an application of hadamads multiplication\nplt.imshow(c1*c3, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nApplying a random mask on the image\n\n\nCode\n#Pr=np.random.randint(1,3,size=c2.shape)\nPr=np.round(1500*np.random.randn(c2.shape[0],c2.shape[1]))\nprint(Pr)\nplt.imshow(c1*Pr, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\n\n\n[[ 1.012e+03  1.068e+03  8.640e+02 ...  1.000e+00  1.083e+03 -2.489e+03]\n [-1.300e+02 -3.080e+02  1.703e+03 ...  1.300e+01  1.191e+03 -1.374e+03]\n [-1.377e+03  1.874e+03 -8.890e+02 ... -3.027e+03  2.181e+03  8.080e+02]\n ...\n [-3.280e+02 -5.700e+01 -2.140e+02 ... -1.006e+03  2.817e+03 -2.685e+03]\n [ 3.019e+03 -1.127e+03 -2.963e+03 ...  9.280e+02 -2.867e+03 -1.213e+03]\n [-7.050e+02 -1.796e+03 -1.075e+03 ...  1.240e+03  1.810e+02  3.230e+02]]\n\n\n\n\n\n\n\n\n\n\n\nCode\n## noising an image using matrix addition\nplt.imshow(c1+Pr, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()\nnoised=c1+Pr\n\n\n\n\n\n\n\n\n\n\n\nCode\n##de-noising an image using matrix operation\nplt.imshow(noised-Pr, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\nplt.show()"
  },
  {
    "objectID": "03-Basic-python.html#application-problems",
    "href": "03-Basic-python.html#application-problems",
    "title": "Module 0 — Introduction to Visualization",
    "section": "Application Problems",
    "text": "Application Problems\nThese problems demonstrate how to apply the libraries above to real-world engineering scenarios.\n\nDigital Communication: Signal Processing\nProblem: Store signal samples, reshape them into time frames, and compute signal statistics (Max, Min, Average).\n\n\nCode\nimport numpy as np\n\n# Signal samples\nsignal = np.array([2.1, 2.5, 2.3, 2.8, 3.0, 2.9])\n\n# Reshape into 2D array (2 frames, 3 samples each)\nsignal_reshaped = signal.reshape(2, 3)\n\n# Signal analysis\nprint(\"Reshaped Signal:\\n\", signal_reshaped)\nprint(\"Maximum Signal Value:\", np.max(signal_reshaped))\nprint(\"Minimum Signal Value:\", np.min(signal_reshaped))\nprint(\"Average Signal Value:\", np.mean(signal_reshaped))\n\n\nReshaped Signal:\n [[2.1 2.5 2.3]\n [2.8 3.  2.9]]\nMaximum Signal Value: 3.0\nMinimum Signal Value: 2.1\nAverage Signal Value: 2.6\n\n\n\n\nAntenna Theory: Radiation Intensity\nProblem: Compute radiation intensity proportional to \\(\\sin(\\theta)\\) for specific angles.\n\n\nCode\n# Angles in radians: 0, pi/4, pi/2, pi\ntheta = np.array([0, np.pi/4, np.pi/2, np.pi])\n\n# Radiation intensity computation\nintensity = np.sin(theta)\n\nprint(\"Angles (radians):\", theta)\nprint(\"Radiation Intensity:\", intensity)\n\n\nAngles (radians): [0.         0.78539816 1.57079633 3.14159265]\nRadiation Intensity: [0.00000000e+00 7.07106781e-01 1.00000000e+00 1.22464680e-16]\n\n\n\n\nCircuit Analysis: Node Voltage\nProblem: Solve the node voltage equation \\(2V - 10 = 0\\) symbolically.\n\n\nCode\nimport sympy as sp\n\n# Define symbolic variable\nV = sp.symbols('V')\n\n# Define and solve equation\nequation = 2*V - 10\nsolution = sp.solve(equation, V)\n\nprint(\"Node Voltage:\", solution)\n\n\nNode Voltage: [5]\n\n\n\n\nFilter Analysis: Frequency Response\nProblem: Visualize the output voltage of a filter against frequency.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nfrequency = np.array([100, 200, 300, 400, 500])\nvoltage = np.array([1.2, 2.1, 3.0, 2.5, 1.8])\n\n# Plotting the response\nplt.plot(frequency, voltage)\nplt.xlabel(\"Frequency (Hz)\")\nplt.ylabel(\"Output Voltage (V)\")\nplt.title(\"Frequency Response of Filter\")\nplt.grid(True) # Adding grid for better readability\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCommunication Channel: Noise Distribution\nProblem: Analyze the distribution of random noise samples using a histogram.\n\n\nCode\n# Noise samples\nnoise = np.array([0.2, -0.1, 0.05, 0.3, -0.25, 0.15, 0.1, -0.05])\n\n# Histogram visualization\nplt.hist(noise, bins=5, edgecolor='black')\nplt.xlabel(\"Noise Amplitude\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Noise Distribution in Channel\")\nplt.show()"
  },
  {
    "objectID": "01-eda-math.html#introduction",
    "href": "01-eda-math.html#introduction",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nModern scientific research and technological innovation are fundamentally data–driven. Across disciplines—engineering, healthcare, economics, environmental sciences, and artificial intelligence—data is the essential raw material that fuels knowledge creation, modelling, and decision making.\nThis introductory module connects mathematical theory with computational practices to prepare mathematicians for research and professional environments where quantitative reasoning and computational thinking play a central role."
  },
  {
    "objectID": "01-eda-math.html#what-is-data",
    "href": "01-eda-math.html#what-is-data",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "What is Data?",
    "text": "What is Data?\nData refers to quantified measurements, observations, records or symbols used to describe objects, events, behaviours, or natural phenomena.\nIn a formal mathematical sense, data can be considered as elements of a set \\[\nX = \\{x_1, x_2, \\dots, x_n\\} \\subset \\mathbb{R}^d\n\\] where each \\(x_i\\) is a \\(d\\)-dimensional vector representing measurable features.\nExamples:\n\nA medical dataset: \\(x_i = (\\text{age}, \\text{height}, \\text{blood pressure})\\)\nThe Iris flower dataset: \\(x_i = (\\text{sepal length}, \\text{sepal width}, \\text{petal length}, \\text{petal width})\\)"
  },
  {
    "objectID": "01-eda-math.html#key-steps-in-data-analysis",
    "href": "01-eda-math.html#key-steps-in-data-analysis",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Key Steps in Data Analysis",
    "text": "Key Steps in Data Analysis\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\nData Collection\nObtaining measurements from instruments, surveys, sensors, or sources\n\n\nData Cleaning\nHandling missing, noisy, duplicated, or inconsistent values\n\n\nData Transformation\nScaling, encoding, normalization and feature engineering\n\n\nExploratory Data Analysis (EDA)\nStatistical summarization and visual discovery of patterns\n\n\nModelling and Interpretation\nMathematical modelling, inference, prediction, and optimization\n\n\nKnowledge Communication\nReports, dashboards, research papers, deployment"
  },
  {
    "objectID": "01-eda-math.html#data-analysis-vs-data-analytics",
    "href": "01-eda-math.html#data-analysis-vs-data-analytics",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Data Analysis vs Data Analytics",
    "text": "Data Analysis vs Data Analytics\n\n\n\n\n\n\n\nData Analysis\nData Analytics\n\n\n\n\nUnderstanding and interpreting datasets\nUsing data to support decisions and predictions\n\n\nInvestigates relationships and structure\nFocuses on outcomes and business/operational value\n\n\nMore mathematical & theory oriented\nMore application & tool oriented\n\n\nExample: hypothesis testing\nExample: customer churn prediction"
  },
  {
    "objectID": "01-eda-math.html#role-of-mathematics-statistics-in-data-analytics",
    "href": "01-eda-math.html#role-of-mathematics-statistics-in-data-analytics",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Role of Mathematics & Statistics in Data Analytics",
    "text": "Role of Mathematics & Statistics in Data Analytics\nMathematics provides:\n\nAbstract structures (sets, functions, spaces, topology)\nOptimization frameworks for machine learning algorithms\nLinear algebra for vectorized computation & dimensionality reduction\nMeasure theory & probability for uncertainty modelling\nInformation theory for data compression and learning\n\nStatistics provides:\n\nSummaries, inference, estimation & hypothesis validation\nUnderstanding variability and uncertainty\nFoundations of experimental design and sampling"
  },
  {
    "objectID": "01-eda-math.html#perspective-shift-for-the-modern-mathematician-strang2019linear",
    "href": "01-eda-math.html#perspective-shift-for-the-modern-mathematician-strang2019linear",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Perspective Shift for the Modern Mathematician (Strang et al. (2019))",
    "text": "Perspective Shift for the Modern Mathematician (Strang et al. (2019))\nTo contribute effectively to data-centric scientific environments, a mathematician must:\n\nMove from closed-form solutions to computational approximation and simulation\nAccept empirical validation instead of purely symbolic proof\nDevelop algorithmic thinking and computational tool fluency\nTranslate abstract models into real-world actionable insight\nEmbrace interdisciplinary collaboration integrating computing and domain knowledge"
  },
  {
    "objectID": "01-eda-math.html#mathematical-formalism",
    "href": "01-eda-math.html#mathematical-formalism",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Mathematical Formalism",
    "text": "Mathematical Formalism\n\nVector Space View of Data\n\\[\nX \\subset \\mathbb{R}^d,\\qquad x_i = (x_{i1}, x_{i2}, \\dots, x_{id})\n\\]\n\n\nNorm-based Measurement\n\\[\n\\|x\\|_1 = \\sum |x_i|,\\qquad \\|x\\|_2 = \\left( \\sum x_i^2 \\right)^{1/2}\n\\]\n\n\nMeasure Theoretic View\n\\[\n(X, \\Sigma, \\mu)\n\\]\nwhere:\n\n\\(X\\) — dataset/sample space\n\\(\\Sigma\\) — σ-algebra of measurable subsets\n\\(\\mu\\) — measure assigning size/probability"
  },
  {
    "objectID": "01-eda-math.html#understanding-exploratory-data-analysis-eda",
    "href": "01-eda-math.html#understanding-exploratory-data-analysis-eda",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Understanding Exploratory Data Analysis (EDA)",
    "text": "Understanding Exploratory Data Analysis (EDA)\n\nWhat is EDA?\nExploratory Data Analysis (EDA) is the process of systematically examining a dataset to discover patterns, identify anomalies, test assumptions, and validate hypotheses using summary statistics and graphical representations. It is the first and most essential step in any analytical or machine learning workflow.\nIn simple terms, EDA tells us the story hidden inside the data.\n\n\nWhy is EDA Important?\nEDA is crucial because:\n\nIt helps understand the structure and characteristics of a dataset before formal modelling.\nIt reveals hidden relationships among variables.\nIt identifies noise, outliers, missing values, and inconsistencies.\nIt guides the selection of appropriate statistical or machine learning models.\nIt prevents incorrect assumptions that could lead to misleading conclusions.\n\nWithout EDA, modelling becomes guesswork rather than a scientifically grounded analysis.\n\n\nCommon Statistical Tools Used in EDA\n\n\n\nStatistical Method\nPurpose\n\n\n\n\nMean, Median, Mode\nCentral behaviour of data\n\n\nVariance, Standard Deviation\nSpread or variability\n\n\nQuantiles & IQR\nRange and distribution behaviour\n\n\nCorrelation measures\nStrength and direction of relationships\n\n\nCovariance matrices\nLinear dependence structure\n\n\nNormality tests\nDistributional assumptions\n\n\n\n\n\nCommon Visualization Tools in EDA\n\n\n\nPlot Type\nPurpose\n\n\n\n\nHistogram\nDistribution and skewness\n\n\nBoxplot\nOutliers and spread\n\n\nScatter Plot\nRelationship between paired variables\n\n\nPairplot (matrix plot)\nAll relationships in multivariate dataset\n\n\nHeatmap\nCorrelation structure\n\n\nViolin Plot / KDE plot\nShape of distribution\n\n\n\nData visualization converts numeric tables into human-understandable patterns and evidence.\n\n\nHow EDA Helps at Different Levels\n\n\n\n\n\n\n\nStakeholder\nBenefit\n\n\n\n\nCommon Individual\nGains clarity about dataset behaviour without technical depth\n\n\nData Scientist / Analyst\nLearns structure, prepares features, selects models, validates assumptions\n\n\nProfessional Decision Maker / Policy Planner\nMakes informed strategic decisions based on evidence instead of intuition\n\n\n\nReal-world example:\n\nIn healthcare analytics, EDA may reveal that a specific symptom correlates strongly with disease severity.\nDecision makers can deploy targeted interventions or allocate resources precisely.\n\nThus, EDA transforms raw data into insight, and insight into action."
  },
  {
    "objectID": "01-eda-math.html#the-iris-dataset-historical-and-scientific-importance",
    "href": "01-eda-math.html#the-iris-dataset-historical-and-scientific-importance",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "The Iris Dataset — Historical and Scientific Importance",
    "text": "The Iris Dataset — Historical and Scientific Importance\nBefore conducting the exploratory analysis, it is important to introduce the dataset that will be used throughout this module: the Iris Flower Dataset, one of the most well-known benchmarks in statistics, machine learning, and pattern recognition.\nThe dataset was first introduced by Sir Ronald Aylmer Fisher (1890–1962) in his groundbreaking 1936 paper titled “The Use of Multiple Measurements in Taxonomic Problems”. Sir R. A. Fisher, widely regarded as the Father of Modern Statistics, pioneered foundational concepts such as maximum likelihood estimation, analysis of variance (ANOVA), and statistical experimental design. His Iris study demonstrated how quantitative measurements could be used to classify biological specimens through multivariate statistics—an idea that evolved into modern machine learning classification techniques.\n\nDescription of the Dataset\nThe Iris dataset consists of 150 samples of iris flowers collected from three species:\n\n\n\nSpecies\nCount\n\n\n\n\nIris setosa\n50\n\n\nIris versicolor\n50\n\n\nIris virginica\n50\n\n\n\nFor each sample, four morphological measurements were recorded (in centimetres):\n\n\n\nFeature\nDescription\n\n\n\n\nSepal Length\nlength of the outer part of the flower\n\n\nSepal Width\nwidth of the outer part\n\n\nPetal Length\nlength of the inner petal\n\n\nPetal Width\nwidth of the inner petal\n\n\n\nThus, the dataset can be represented mathematically as: \\[\nX = \\{x_1, x_2, \\dots, x_{150}\\} \\subset \\mathbb{R}^4\n\\] where each vector \\[\nx_i = (\\text{sepal length},\\ \\text{sepal width},\\ \\text{petal length},\\ \\text{petal width})\n\\]\nThe corresponding class label \\[\ny_i \\in \\{\\text{setosa}, \\text{versicolor}, \\text{virginica}\\}\n\\]\n\n\nWhy is the Iris Dataset Important?\n\nServes as an ideal educational dataset for demonstrating statistical and machine learning concepts.\nExhibits distinct geometric separation between classes, inspiring early classification algorithms.\nProvides simple structure yet complex enough for real-world pattern recognition.\nForms the basis for classical methods such as Linear Discriminant Analysis (LDA) introduced by Fisher himself.\n\n\n\nIris Dataset and EDA\nThe Iris dataset is particularly suited for Exploratory Data Analysis because:\n\nIt contains both numeric features and categorical labels.\nIt enables visual analysis of relationships between pairs of variables.\nIt reveals distributional differences across species.\nIt supports understanding of dimensionality reduction and classification.\n\nWith this context, we now proceed to perform a detailed EDA of the dataset."
  },
  {
    "objectID": "01-eda-math.html#exploratory-data-analysis-with-the-iris-dataset",
    "href": "01-eda-math.html#exploratory-data-analysis-with-the-iris-dataset",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Exploratory Data Analysis with the Iris Dataset",
    "text": "Exploratory Data Analysis with the Iris Dataset\nA glimps of the datset is shown below:\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load iris dataset\ndf = sns.load_dataset(\"iris\")\n\n# Display first rows\ndf.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nA summary of the dataset is shown below:\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nPairwise Relationships\n\n\n\nCode\nsns.pairplot(df, hue='species')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFeature Distributions\n\n\n\nCode\ndf.hist(figsize=(8,6), bins=10)\nplt.suptitle(\"Distribution of Features in Iris Dataset\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCorrelation Heatmap\n\n\n\nCode\nsns.heatmap(df.corr(numeric_only=True), annot=True)\nplt.title(\"Correlation Matrix\")\nplt.show()"
  },
  {
    "objectID": "01-eda-math.html#reflections",
    "href": "01-eda-math.html#reflections",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "Reflections",
    "text": "Reflections\n\nHow do norms relate to similarity and distance in machine learning algorithms?\nHow does EDA support model selection and problem formulation?\nWhich relationships among the Iris features appear strongest from the heatmap?"
  },
  {
    "objectID": "01-eda-math.html#references",
    "href": "01-eda-math.html#references",
    "title": "Module 1 — Data Foundations and Exploratory Data Analysis",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "02-ml-core.html#introduction",
    "href": "02-ml-core.html#introduction",
    "title": "Module 2 — Machine Learning Core",
    "section": "Introduction",
    "text": "Introduction\nMachine Learning (ML) is a scientific discipline that enables systems to learn patterns from data and make decisions without explicitly defined rules. From a mathematical perspective, machine learning is grounded in optimization theory, statistical inference, multivariable calculus, probability, linear algebra, and functional analysis.\nThis module aims to connect the abstract mathematical foundations with computational implementations, guiding mathematicians to appreciate machine learning not only as an applied tool, but as a modern extension of optimization and function approximation theory."
  },
  {
    "objectID": "02-ml-core.html#what-is-machine-learning",
    "href": "02-ml-core.html#what-is-machine-learning",
    "title": "Module 2 — Machine Learning Core",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nMachine learning seeks to approximate an unknown functional relationship between input features \\(X\\) and outputs \\(Y\\), based on observed sample data. Formally, given: \\[\nX = \\{x_1, x_2, \\dots, x_n\\} \\subset \\mathbb{R}^d,\\qquad\nY = \\{y_1, y_2, \\dots, y_n\\}\n\\]\nThe goal is to learn a function: \\[\nf_\\theta : \\mathbb{R}^d \\rightarrow Y\n\\]\nsuch that the prediction error is minimized: \\[\n\\theta^\\* = \\arg \\min_{\\theta} L(f_\\theta(X), Y)\n\\]\nwhere \\(L\\) is a loss function, representing the discrepancy between predictions and truth."
  },
  {
    "objectID": "02-ml-core.html#key-types-of-machine-learning",
    "href": "02-ml-core.html#key-types-of-machine-learning",
    "title": "Module 2 — Machine Learning Core",
    "section": "Key Types of Machine Learning",
    "text": "Key Types of Machine Learning\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nSupervised Learning\nLearn mapping from features to target labels\nClassification & Regression\n\n\nUnsupervised Learning\nLearn patterns and structure without labels\nClustering & PCA\n\n\nReinforcement Learning\nLearn behaviour through rewards\nRobotics & Game AI"
  },
  {
    "objectID": "02-ml-core.html#role-of-mathematics-in-machine-learning",
    "href": "02-ml-core.html#role-of-mathematics-in-machine-learning",
    "title": "Module 2 — Machine Learning Core",
    "section": "Role of Mathematics in Machine Learning",
    "text": "Role of Mathematics in Machine Learning\n\n\n\n\n\n\n\nMathematical Component\nContribution\n\n\n\n\nMultivariable Calculus\nOptimization through gradients and updates\n\n\nLinear Algebra\nVectorization, matrix operations, feature transformations\n\n\nProbability & Statistics\nInference, uncertainty modelling, distributions\n\n\nOptimization Theory\nConvergence guarantees and algorithm evaluation\n\n\nMeasure Theory\nFormal understanding of learning over spaces\n\n\n\nMachine learning problems are fundamentally optimization problems in high-dimensional spaces."
  },
  {
    "objectID": "02-ml-core.html#gradient-descent-core-optimization-engine",
    "href": "02-ml-core.html#gradient-descent-core-optimization-engine",
    "title": "Module 2 — Machine Learning Core",
    "section": "Gradient Descent — Core Optimization Engine",
    "text": "Gradient Descent — Core Optimization Engine\nThe optimization goal is to find parameters \\(\\theta\\) that minimize the loss function \\(L(\\theta)\\): \\[\n\\theta^\\* = \\arg \\min_{\\theta} L(\\theta)\n\\]\nGradient descent updates parameters iteratively: \\[\n\\theta_{k+1} = \\theta_k - \\eta \\nabla_\\theta L(\\theta_k)\n\\]\nWhere: - \\(\\eta\\) — learning rate (step size) - \\(\\nabla_\\theta L\\) — gradient of loss with respect to parameters\n\nExample Loss Function (Mean Squared Error)\n\\[\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f_\\theta(x_i))^2\n\\]\nRegularization helps avoid overfitting by penalizing overly complex models:\n\n\n\\(L_2\\) Regularization:\n\\[\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - f_\\theta(x_i))^2 + \\lambda \\|\\theta\\|_2^2\n\\]\n\n\n\\(L_1\\) Regularization:\n\\[\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - f_\\theta(x_i))^2 + \\lambda \\|\\theta\\|_1\n\\]"
  },
  {
    "objectID": "02-ml-core.html#linear-algebra-in-machine-learning",
    "href": "02-ml-core.html#linear-algebra-in-machine-learning",
    "title": "Module 2 — Machine Learning Core",
    "section": "Linear Algebra in Machine Learning",
    "text": "Linear Algebra in Machine Learning\nMachine learning models rely heavily on matrix-vector operations:\n\\[\ny = X\\theta\n\\]\nFor \\(n\\) samples and \\(d\\) features:\n\\[\nX \\in \\mathbb{R}^{n \\times d},\\quad \\theta \\in \\mathbb{R}^d,\\quad y \\in \\mathbb{R}^n\n\\]\nDimensionality reduction & feature extraction operate via eigenvalue decomposition: \\[\nA v = \\lambda v\n\\]\nThis provides the basis for PCA and optimization-based feature representation."
  },
  {
    "objectID": "02-ml-core.html#example-ml-task-iris-classification",
    "href": "02-ml-core.html#example-ml-task-iris-classification",
    "title": "Module 2 — Machine Learning Core",
    "section": "Example ML Task — Iris Classification",
    "text": "Example ML Task — Iris Classification\nWe now apply machine learning to classify the three flower species using logistic regression.\n\n\nCode\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression(max_iter=200)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n\nAccuracy: 1.0\nConfusion Matrix:\n [[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Key Topics & Detailed Breakdown",
    "section": "",
    "text": "Data science for Mathematics Tribe\n\n    Opening Session\n\n  \n\n\n\n\n\n\nThis page provides the structured schedule and detailed mathematical content for the 4-hour intensive session.\n\n\n\n\n\n\n\n\n\nTime Slot\nModule Title\nCore Mathematical Focus\nComputational Tools\n\n\n\n\n0:00 - 1:00\nData Foundations and EDA\nDescriptive Statistics, Set Theory, Measure Theory (brief), \\(\\mathbf{L_1}\\)/\\(\\mathbf{L_2}\\) Norms, Data Transformation Functions.\nPandas, NumPy, Visualisation Libraries (e.g., Matplotlib/ggplot2)\n\n\n1:00 - 2:00\nMachine Learning Core\nMultivariable Calculus, Gradient Descent (Calculus of Variations), Linear Algebra (Vector Spaces), Regularization (Penalties and Constraint Sets).\nScikit-learn, TensorFlow/PyTorch (High-level overview)\n\n\n2:00 - 3:00\nDimensionality Reduction\nEigenvalue/Eigenvector Decomposition, Singular Value Decomposition (SVD), Matrix Algebra.\nScikit-learn (PCA implementation), NumPy/SciPy\n\n\n3:00 - 4:00\nResearch Frontiers\nGraph Theory (TDA), Topology, Information Theory, Optimization Theory (Advanced topics).\nSpecialized Python libraries (e.g., Giotto-TDA, SHAP for XAI)"
  }
]