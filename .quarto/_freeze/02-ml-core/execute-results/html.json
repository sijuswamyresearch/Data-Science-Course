{
  "hash": "fa03d29aaf487571dd5eec5600d4308e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Module 2 — Machine Learning Core\"\nsubtitle: \"Mathematics of Learning Systems, Optimization, and Computational Perspective\"\nformat:\n  html:\n    theme: yeti\n    toc: true\n    toc-depth: 3\n    smooth-scroll: true\n    code-fold: true\nauthor: \"Siju Swamy\"\nexecute:\n  echo: true\n  warning: false\n  message: false\njupyter: python3\n---\n\n<div style=\"width: 100%; text-align: center;\">\n  <img src=\"M2_ML_core.png\" \n       alt=\"A descriptive text for the image\" \n       style=\"width: 100%; height: auto; max-width: 1000px; display: block; margin: 0 auto;\">\n</div>\n\n## Introduction\n\nMachine Learning (ML) is a scientific discipline that enables systems to learn patterns\nfrom data and make decisions without explicitly defined rules. From a mathematical perspective,\nmachine learning is grounded in optimization theory, statistical inference, multivariable calculus,\nprobability, linear algebra, and functional analysis.\n\nThis module aims to connect the abstract mathematical foundations with computational implementations,\nguiding mathematicians to appreciate machine learning not only as an applied tool, but as\na modern extension of optimization and function approximation theory.\n\n## What is Machine Learning?\n\nMachine learning seeks to approximate an unknown functional relationship between input features $X$\nand outputs $Y$, based on observed sample data. Formally, given:\n$$\nX = \\{x_1, x_2, \\dots, x_n\\} \\subset \\mathbb{R}^d,\\qquad\nY = \\{y_1, y_2, \\dots, y_n\\}\n$$\n\nThe goal is to learn a function:\n$$\nf_\\theta : \\mathbb{R}^d \\rightarrow Y\n$$\n\nsuch that the **prediction error is minimized**:\n$$\n\\theta^\\* = \\arg \\min_{\\theta} L(f_\\theta(X), Y)\n$$\n\nwhere $L$ is a **loss function**, representing the discrepancy between predictions and truth.\n\n---\n\n## Key Types of Machine Learning\n\n| Type | Description | Example |\n|-------|-----------|---------|\n| Supervised Learning | Learn mapping from features to target labels | Classification & Regression |\n| Unsupervised Learning | Learn patterns and structure without labels | Clustering & PCA |\n| Reinforcement Learning | Learn behaviour through rewards | Robotics & Game AI |\n\n---\n\n## Role of Mathematics in Machine Learning\n\n| Mathematical Component | Contribution |\n|------------------------|--------------|\n| **Multivariable Calculus** | Optimization through gradients and updates |\n| **Linear Algebra** | Vectorization, matrix operations, feature transformations |\n| **Probability & Statistics** | Inference, uncertainty modelling, distributions |\n| **Optimization Theory** | Convergence guarantees and algorithm evaluation |\n| **Measure Theory** | Formal understanding of learning over spaces |\n\nMachine learning problems are fundamentally **optimization problems in high-dimensional spaces**.\n\n---\n\n## Gradient Descent — Core Optimization Engine\n\nThe optimization goal is to find parameters $\\theta$ that minimize the loss function $L(\\theta)$:\n$$\n\\theta^\\* = \\arg \\min_{\\theta} L(\\theta)\n$$\n\nGradient descent updates parameters iteratively:\n$$\n\\theta_{k+1} = \\theta_k - \\eta \\nabla_\\theta L(\\theta_k)\n$$\n\nWhere:\n- $\\eta$ — learning rate (step size)\n- $\\nabla_\\theta L$ — gradient of loss with respect to parameters\n\n### Example Loss Function (Mean Squared Error)\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f_\\theta(x_i))^2\n$$\n\nRegularization helps avoid overfitting by penalizing overly complex models:\n\n### $L_2$ Regularization:\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - f_\\theta(x_i))^2 + \\lambda \\|\\theta\\|_2^2\n$$\n\n### $L_1$ Regularization:\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - f_\\theta(x_i))^2 + \\lambda \\|\\theta\\|_1\n$$\n\n\n## Linear Algebra in Machine Learning\n\nMachine learning models rely heavily on matrix-vector operations:\n\n$$\ny = X\\theta\n$$\n\nFor $n$ samples and $d$ features:\n\n$$\nX \\in \\mathbb{R}^{n \\times d},\\quad \\theta \\in \\mathbb{R}^d,\\quad y \\in \\mathbb{R}^n\n$$\n\nDimensionality reduction & feature extraction operate via eigenvalue decomposition:\n$$\nA v = \\lambda v\n$$\n\nThis provides the basis for PCA and optimization-based feature representation.\n\n\n\n## Example ML Task — Iris Classification\n\nWe now apply machine learning to classify the three flower species using logistic regression.\n\n::: {#4338d7b6 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression(max_iter=200)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0\nConfusion Matrix:\n [[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02-ml-core_files"
    ],
    "filters": [],
    "includes": {}
  }
}